{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "metadata": {
        "id": "FC8i70qtU9WF"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code, last update by OpenAI on 2019, uses tensorflow 1.x\n",
        "Operation/modules/things ```removed``` in tensorflow 2.x:\n",
        "- tf.variable_scope\n",
        "- tf.get_variable\n",
        "- tf.rsqrt\n",
        "- tf.contrib\n",
        "\n",
        "People [suggest](https://stackoverflow.com/questions/63350105/how-to-alter-gpt-2-code-to-work-with-tensorflow-2-0) that you should not use beyond Tensorflow=1.15 with this repo, because migration from 1.x to 2.x is\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        ".\n",
        "\n",
        "non-trivial."
      ],
      "metadata": {
        "id": "YeWxvmXM3jbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "# Load pre-trained model and tokenizer\n",
        "model_name = \"gpt2\"  # You can choose from different sizes: gpt2, gpt2-medium, gpt2-large, gpt2-xl\n",
        "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Encode input text\n",
        "input_text = \"Once upon a time\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
        "\n",
        "# Generate text\n",
        "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "\n",
        "# Decode and print the result\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "print(generated_text)\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "SvecrctO7-mo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's dig into HuggingFace ```transformers``` library (idea from ChatGPT):\n",
        "- [Main hole](https://github.com/huggingface/transformers) to dig\n",
        "- Read the Readme. For example it discusses the benefits of using HF transformers, and mentions \"Move a single model between TF2.0/PyTorch/JAX frameworks at will\". What is this so-much-repeated JAX and why we haven't dug into it yet? [Flax](https://flax.readthedocs.io/en/latest/): neural networks with JAX\n",
        "\n",
        "- [Models](https://github.com/huggingface/transformers/tree/main/src/transformers/models) --> [gpt2](https://github.com/huggingface/transformers/tree/main/src/transformers/models/gpt2) -->[TF 2.0](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_tf_gpt2.py) and [PyTorch](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py) impl's.\n",
        "- gpt2 -> convert_gpt2_original_tf_checkpoint_to_pytorch.py\n",
        "Script for converting TensorFlow GPT2 checkpoint to PyTorch:\n",
        "-- ```parser = argparse.ArgumentParser()```\n",
        "This creates an ArgumentParser object that will handle command-line arguments."
      ],
      "metadata": {
        "id": "wumjENxa7nKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's dive deep into TF implementation. Interestingly the ```causal_attention_mask``` [function](https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_tf_gpt2.py#:~:text=causal_attention_mask) is copy-pasted from OpenAI gpt2 source code which we studied below. I [digged](https://stackoverflow.com/questions/18220650/github-link-to-function-in-source) to find a way to point to function definition instead of passing the line numbers as files change over time."
      ],
      "metadata": {
        "id": "LMwQj-j-b4uF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "def conv1d(x, scope, nf, *, w_init_stdev=0.02):\n",
        "    with tf.variable_scope(scope):\n",
        "        *start, nx = shape_list(x)\n",
        "        w = tf.get_variable('w', [1, nx, nf], initializer=tf.random_normal_initializer(stddev=w_init_stdev))\n",
        "        b = tf.get_variable('b', [nf], initializer=tf.constant_initializer(0))\n",
        "        c = tf.reshape(tf.matmul(tf.reshape(x, [-1, nx]), tf.reshape(w, [-1, nf]))+b, start+[nf])\n",
        "        return c\n",
        "```\n",
        "In standard 1D convolution, you would apply a filter that \"slides\" across the input with a defined stride and performs local dot products. This function instead treats it more like a fully connected layer over the last dimension of x.\n",
        "\n",
        "If you wanted a true 1D convolution, you'd typically use a function like tf.nn.conv1d or tf.keras.layers.Conv1D."
      ],
      "metadata": {
        "id": "A3fDv3PJUQts"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Lhy-P52IQlpe"
      },
      "outputs": [],
      "source": [
        "def bool_attention_mask(nd, ns):\n",
        "    \"\"\"1's in the lower triangle, counting from the lower right corner.\n",
        "\n",
        "    Same as tf.matrix_band_part(tf.ones([nd, ns]), -1, ns-nd), but doesn't produce garbage on TPUs.\n",
        "    \"\"\"\n",
        "    i = tf.range(nd)[:,None]\n",
        "    j = tf.range(ns)\n",
        "    m = i >= j - ns + nd  # (i, j) below and on the main diagonal <--> i - nd >= j - ns\n",
        "    return m\n",
        "'''\n",
        "args after * are keyword_only and should be called using the keyword syntax, not positionally, and are required\n",
        "'''\n",
        "m = bool_attention_mask(3, 4)\n",
        "assert m.dtype == tf.bool\n",
        "'''\n",
        "# do not use dtype for bool_attention_mask.\n",
        "def attention_mask(nd, ns, *, dtype):\n",
        "    return tf.cast(bool_attention_mask(nd, ns), dtype)\n",
        "\n",
        "m = attention_mask(3, 4, dtype=tf.float32)\n",
        "# TypeError: bool_attention_mask() missing 1 required keyword-only argument: 'dtype'\n",
        "'''\n",
        "def attention_mask(ns, nd, *, dtype):\n",
        "    return tf.cast(bool_attention_mask(nd, ns), dtype)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_states(x, n):\n",
        "    *start, m = x.shape\n",
        "    return tf.reshape(x, start + [n, m//n])\n",
        "\n",
        "x = tf.range(12)\n",
        "assert x.shape == (12,) and x.shape == (12) and type(x.shape) == tf.TensorShape and x.shape.as_list() == [12]\n",
        "print(type(x.shape))\n",
        "*start, m = x.shape\n",
        "assert start == [] and m == 12\n",
        "'''\n",
        "start, m = x.shape is wrong:\n",
        "# ValueError: not enough values to unpack (expected 2, got 1)\n",
        "'''\n",
        "x = tf.reshape(x, (2,6))\n",
        "# x.shape #TensorShape([2,6])\n",
        "*start, m = x.shape\n",
        "assert start == [2] and tf.reshape(x, start + [2,3]).shape == [2, 2, 3]\n",
        "assert tf.reduce_all(split_states(x, 2) == tf.reshape(x, start + [2,3]))\n",
        "'''\n",
        "*a, *b, m = tf.range(12).shape\n",
        "SyntaxError: multiple starred expressions in assignment\n",
        "In Python, you can only use one starred expression (*) when unpacking elements from a sequence.\n",
        "Multiple starred expressions (e.g., *a, *b, m) are not allowed, which is why you're getting a SyntaxError.\n",
        "'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "3gD__UNhZHE2",
        "outputId": "be9ef133-0f68-40bc-8b1d-8c0f02e8a7f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tensorflow.python.framework.tensor_shape.TensorShape'>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n*a, *b, m = tf.range(12).shape\\nSyntaxError: multiple starred expressions in assignment\\nIn Python, you can only use one starred expression (*) when unpacking elements from a sequence.\\nMultiple starred expressions (e.g., *a, *b, m) are not allowed, which is why you're getting a SyntaxError.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_states(x):\n",
        "    *start, a, b = x.shape #unpacking shape sequence\n",
        "    return tf.reshape(x, start + [a*b])\n",
        "# see split_states() block for unpacking the shape using *start"
      ],
      "metadata": {
        "id": "OZIO6HUnfn3k"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def shape_list(x):\n",
        "    static = x.shape.as_list()\n",
        "    dynamic = tf.shape(x)\n",
        "    return [dynamic[i] if s is None else s for i, s in enumerate(static)]\n",
        "\n",
        "x = tf.range(12)\n",
        "x = tf.reshape(x, [2, 6])\n",
        "static = x.shape.as_list()\n",
        "\n",
        "x = tf.reshape(x, (3,4))\n",
        "assert tf.shape(x).dtype == tf.int32\n",
        "type(tf.shape(x))\n",
        "\n",
        "'''\n",
        "# in tensorflow 1.x\n",
        "# x = tf.placeholder(tf.float32, shape=[None, 10])  # `None` indicates an unknown batch size\n",
        "'''\n",
        "x = tf.keras.Input(shape=(10,))\n",
        "assert x.shape == (None, 10)\n",
        "# print(tf.shape(x))\n",
        "'''\n",
        "ValueError: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\n",
        "\n",
        "x = Input(...)\n",
        "...\n",
        "tf_fn(x)  # Invalid.\n",
        "\n",
        "What you should do instead is wrap `tf_fn` in a layer:\n",
        "\n",
        "class MyLayer(Layer):\n",
        "    def call(self, x):\n",
        "        return tf_fn(x)\n",
        "\n",
        "x = MyLayer()(x)\n",
        "\n",
        "'''"
      ],
      "metadata": {
        "id": "zMjAsoHKncLl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "92d805c2-8c2e-42cf-ca6c-c8cf2bbdc23d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nValueError: A KerasTensor cannot be used as input to a TensorFlow function. A KerasTensor is a symbolic placeholder for a shape and dtype, used when constructing Keras Functional models or Keras Functions. You can only use it as input to a Keras layer or a Keras operation (from the namespaces `keras.layers` and `keras.operations`). You are likely doing something like:\\n\\nx = Input(...)\\n...\\ntf_fn(x)  # Invalid.\\n\\nWhat you should do instead is wrap `tf_fn` in a layer:\\n\\nclass MyLayer(Layer):\\n    def call(self, x):\\n        return tf_fn(x)\\n\\nx = MyLayer()(x)\\n\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TensorFlow 1.x:\n",
        "\n",
        "tf.placeholder is used to create a placeholder with a shape where the first dimension (the batch size) is not defined (None). This is common in TensorFlow 1.x where the batch size is typically dynamic.\n",
        "\n",
        "TensorFlow 2.x:\n",
        "\n",
        "Using tf.keras.Input is a common way to work with dynamic shapes in TensorFlow 2.x, especially when defining models. For general tensors, you can work with them directly without placeholders."
      ],
      "metadata": {
        "id": "y_hpSRL_rbpb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def attn(x, scope, n_state, *, past, hparams):\n",
        "    assert x.shape.ndims == 3 # [batch, sequence, features] ([B, T, C])\n",
        "    assert n_state % hparams.n_head == 0\n",
        "    if past is not None:\n",
        "        assert past.shape.ndims == 5 # [batch, 2, heads, sequence, features]\n",
        "\n",
        "    def split_heads(x):\n",
        "        '''\n",
        "        [batch, sequence, features] -> [batch, head, sequence, features]\n",
        "        '''\n",
        "        return tf.transpose(split_states(x, hparams.n_head), (0, 2, 1, 3))\n",
        "\n",
        "    def merge_heads(x):\n",
        "        '''\n",
        "        [batch, head, sequence, features] -> [batch, sequence, features]\n",
        "        '''\n",
        "        return merge_states(tf.transpose(x, (0, 2, 1, 3)))\n",
        "\n",
        "    def mask_attn_weights(w):\n",
        "        # w.shape == [batch, heads, dst_sequence, src_sequence]\n",
        "        _, _, nd, ns = shape_list(w)\n",
        "        b = attention_mask(nd, ns, dtype=w.dtype)\n",
        "        b = tf.reshape(b, [1,1,nd,ns])\n",
        "        w = w*b - tf.cast(1e10, w.dtype)*(1-b) # 0 in mask -> not attend to it\n",
        "        # * is element-wise multiplication\n",
        "        return w\n",
        "\n",
        "    def multihead_attn(q, k, v):\n",
        "        # q, k, v shapes = [batch, heads, sequence, features]\n",
        "        w = tf.matmul(q, k, transpose_b=True)\n",
        "        w = w * tf.rsqrt(tf.cast(v.shape[-1].value, w.dtype))\n"
      ],
      "metadata": {
        "id": "ARUVoloDU5bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "digger [post](https://stackoverflow.com/questions/34192229/efficient-element-wise-multiplication-of-a-matrix-and-a-vector-in-tensorflow):\n",
        "\n",
        "\\* vs tf.multiply() vs \\_\\_mul\\_\\_()\n",
        "\n",
        "tf.matmul [docs](https://www.tensorflow.org/api_docs/python/tf/linalg/matmul):\n",
        "\n",
        "If one or both of the matrices contain a lot of zeros, a more efficient multiplication algorithm can be used by setting the corresponding a_is_sparse or b_is_sparse flag to True. These are False by default. This optimization is only available for plain matrices (rank-2 tensors) with datatypes bfloat16 or float32."
      ],
      "metadata": {
        "id": "NRyEvErylw-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.rsqrt(3.0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "F_qScpTeZE8t",
        "outputId": "1f7db5ae-9d35-4118-c840-db4f52f3dbe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'tensorflow' has no attribute 'rsqrt'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-91-54e7765be700>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrsqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'rsqrt'"
          ]
        }
      ]
    }
  ]
}