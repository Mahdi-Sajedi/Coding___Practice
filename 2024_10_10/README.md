[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://huggingface.co/papers/2402.17764)
shumingma
Paper author
Feb 28

Unfortunately, the conversion or post-training quantization from existing LLMs doesn't help. This is why we train the models from scratch
