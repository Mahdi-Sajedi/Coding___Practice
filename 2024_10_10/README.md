[samadi podcast](https://m.youtube.com/watch?v=pWCIFz2t8a8)



[The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits](https://huggingface.co/papers/2402.17764)
shumingma
Paper author
Feb 28

Unfortunately, the conversion or post-training quantization from existing LLMs doesn't help. This is why we train the models from scratch


[2nd link](https://huggingface.co/1bitLLM)
